{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dyhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML=open(r'./keras_dataset/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져옵니다.\n",
    "\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "# content_text = re.sub(r'\\n', '', parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the input corpus \n",
    "# do the tokenize to sentence unit\n",
    "sent_text=sent_tokenize(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the corpus of the set of sentences\n",
    "# do the tokenize to word unit\n",
    "result=[]\n",
    "result=[word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batman', 0.8164232969284058), ('woman', 0.8112325072288513), ('hoffman', 0.7885738611221313), ('ekman', 0.7877068519592285), ('lehman', 0.7624384164810181), ('salman', 0.7602828145027161), ('kahneman', 0.7593111395835876), ('shaman', 0.7590751647949219), ('foreman', 0.7556301951408386), ('manju', 0.7490721344947815)]\n"
     ]
    }
   ],
   "source": [
    "a=model.wv.most_similar(\"man\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electrolux', 0.8036383390426636), ('electrolyte', 0.7956132292747498), ('electro', 0.7949458956718445), ('electric', 0.7691299319267273), ('electroshock', 0.7676375508308411), ('electrochemical', 0.7625319361686707), ('electrogram', 0.759557843208313), ('airbus', 0.7510166168212891), ('electronic', 0.7487788200378418), ('fishing', 0.7450719475746155)]\n"
     ]
    }
   ],
   "source": [
    "a=model.wv.most_similar(\"electrofishing\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handy_env",
   "language": "python",
   "name": "handy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
